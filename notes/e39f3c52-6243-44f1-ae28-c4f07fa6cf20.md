---
id: e39f3c52-6243-44f1-ae28-c4f07fa6cf20
title: High Performance NLP
desc: ''
updated: 1606048459146
created: 1605878472921
parent: 1c37314a-8278-4077-b7e4-724882b94215
children: []
fname: papers.tutorials.highperformancenlp
hpath: papers.tutorials.highperformancenlp
---
# High performance NLP tutorial

<https://slideslive.com/38940826>

![](emnlp2020-notes/assets/images/2020-11-20-14-26-58.png)

Balance between theoretically promising and practically achievable models

1. Techniques :
   - Distillation
   - Quantization
   - Pruning
2. Efficient attention
   - Data independent patterns
   - Data dependent patterns
   - Alternative attention mechanisms
   - Recurrence
3. Case studies
4. Scaling in practice
   - Scaling laws of LMs
   - Parallelism techniques
   - Methods to reduce memory footprint
   - Mixture of experts

## Fundamentals

![](emnlp2020-notes/assets/images/2020-11-20-14-38-33.png)
![](emnlp2020-notes/assets/images/2020-11-20-14-40-06.png)

![](emnlp2020-notes/assets/images/2020-11-20-14-41-24.png)
![](emnlp2020-notes/assets/images/2020-11-20-14-42-30.png)

![](emnlp2020-notes/assets/images/2020-11-20-14-44-32.png)
![](emnlp2020-notes/assets/images/2020-11-20-14-44-49.png)

## Core techniques

### Distillation

Use soft labels to convey uncertainty / nuances to student model

When should distillation happen? Pretraining, finetuning, both?

#### Pretraining

DistilBERT, distillation at pretraining. Less layers

1. Take pretrained BERT base as teacher
2. Keep its bottom 6 layers only, shallow student
3. Continue training the 6 layer via knowledge distillation : use teacher probabilities as guide

MobileBERT, also distil at pretraining. Same number of layers as teacher but smaller embedding size.
Encourage at each layer alignment of :

- Feature maps (challenge : different dims), up and down projections to 512 in between dims
- Attention maps : want the student to know where to focus attention, minimize KL divergence between attention distributions
  ![](emnlp2020-notes/assets/images/2020-11-20-14-57-08.png)

Comparable accuracy on GLUE, 4x smaller, 6x faster than BERT Base

#### Fine-tuning

![](emnlp2020-notes/assets/images/2020-11-20-14-59-17.png)

Useful when we have large amounts of in-domain task data without gold labels, can use teacher labels

#### Both

TinyBERT
![](emnlp2020-notes/assets/images/2020-11-20-15-00-20.png)

### Quantization

![](emnlp2020-notes/assets/images/2020-11-20-15-41-40.png)

When to quantize?

- Most convenient : post-training, often drop in accuracy
- Even if using fine-tuning step after, can be unsatisfactory

Solution : Quantization-aware training or Simulated quantization

- Forward pass : As if quantized
- Backward pass : Normal

Models that quantize weights and also activations :
![](emnlp2020-notes/assets/images/2020-11-20-15-46-40.png)

- Q8BERT : same forward / backward strategy as above
- Q-BERT : Each layer has own quantization, group precision to save on memory
- TernaryBERT : [papers.transformers.day1](948323c7-b75c-42e8-bb35-849463e56ea3) -1, 0 or 1. Distillation + quantization 15x decrease in size
  2 bits per parameter, 2 students
  ![](emnlp2020-notes/assets/images/2020-11-20-15-53-45.png)

Loss :

- Student full precision diff with quantized student
- Distillation loss, cross-entropy studen teacher
- MSE distance between hidden layers and attention scores

### Pruning

Not only more efficient, can be seen as tech to improve generalization

![](emnlp2020-notes/assets/images/2020-11-20-17-30-23.png)

Early ideas : prune based on second order derivatives

Why do we need to start with large network and downsize instead of training the small one?

#### Lottery ticket hypothesis

The larger the network, the more lottery tickets (possible subnetworks) and therefore the larger the chances of finding one that performs well.

The smaller model can be trained in isolation.

![](emnlp2020-notes/assets/images/2020-11-20-17-35-55.png)

Iterative pruning is efficient, iterate pruning + fine-tuning

In practice : fails on very deep networks where it cannot find a winning ticket

Revisited more stable network :
![](emnlp2020-notes/assets/images/2020-11-20-17-38-40.png)

NMT :
![](emnlp2020-notes/assets/images/2020-11-20-17-40-17.png)

#### Movement pruning

Apply pruning during fine-tuning to reuse pretrained models

Magnitude pruning doesn't perform well in transfer learning => movement pruning

Every parameter is associated with a learned importance score

![](emnlp2020-notes/assets/images/2020-11-20-17-45-35.png)

95% accuracy of BERT base while keeping 15% of its parameters

Previous methods : unstructured pruning, mostly memory saving since sparse compute poorly supported

Hardware lottery : the methods that are most adapted to current hardware become the popular ones
![](emnlp2020-notes/assets/images/2020-11-20-17-46-43.png)

## Efficient attention

Quadratic bottleneck in length, problem for :

- Long-range dependencies
- Character level models
- Speech processing
- High resolution image processing

![](emnlp2020-notes/assets/images/2020-11-20-17-50-28.png)

### Data independent patterns

![](emnlp2020-notes/assets/images/2020-11-20-18-00-23.png)

Fixed patterns on attention matrix and sparsify

- Blockwise patterns : reduction to being quadratic in block length
  ![](emnlp2020-notes/assets/images/2020-11-20-18-03-55.png)
- Strided patterns : reduction by constant factor
  ![](emnlp2020-notes/assets/images/2020-11-20-18-04-31.png)
- Diagonal patterns :
  ![](emnlp2020-notes/assets/images/2020-11-20-18-05-12.png)
- Random patterns :
  ![](emnlp2020-notes/assets/images/2020-11-20-18-05-39.png)
- Global attention :
  ![](emnlp2020-notes/assets/images/2020-11-20-18-05-58.png)
- Combination of patterns
  ![](emnlp2020-notes/assets/images/2020-11-20-18-06-20.png)

### Data dependent patterns

![](emnlp2020-notes/assets/images/2020-11-20-18-01-39.png)

- Buckets
  ![](emnlp2020-notes/assets/images/2020-11-20-18-07-14.png)
  Reformer : use LSH and look at only items in same bucket during attention
  ![](emnlp2020-notes/assets/images/2020-11-20-18-08-26.png)
  Clustering over activation : Routing transformer, clustered attention
- Sorting and blocking : Sort and restrict attention to only current block or neighbours : Sinkhorn Transformer
- Compression :
  Attention matrix has been empirically demonstrated to be approximately low rank
  ![](emnlp2020-notes/assets/images/2020-11-20-18-11-21.png)

### Kernels and alternative attention mechanisms

![](emnlp2020-notes/assets/images/2020-11-20-18-02-24.png)

- Kernels : Simplify attention $\\phi(Q, K) =exp(\\frac{Q^T K}{\\sqrt{d}})$ into decomposable kernel $\\phi(Q)^T \\phi(K)$
  ![](emnlp2020-notes/assets/images/2020-11-20-18-14-05.png)
  If we can do this, no need to pay quadratic price => linear
  ![](emnlp2020-notes/assets/images/2020-11-20-18-16-59.png)

Proposed kernels :
![](emnlp2020-notes/assets/images/2020-11-20-18-18-17.png)
Random features

- Performer
  ![](emnlp2020-notes/assets/images/2020-11-20-18-19-09.png)

- Synthesizers
  ![](emnlp2020-notes/assets/images/2020-11-20-18-20-56.png)

### Recurrence

![](emnlp2020-notes/assets/images/2020-11-20-18-02-41.png)

- Transformer-XL : split sequences to different chunks, process them independently. Segment-level recurrence mechanism : no need to backprop through previous segment
  ![](emnlp2020-notes/assets/images/2020-11-20-18-22-35.png)

- Compressive transformers :
  ![](emnlp2020-notes/assets/images/2020-11-20-18-23-57.png)

### How do they compare?

- Benchmark : long range arena
  ![](emnlp2020-notes/assets/images/2020-11-20-18-24-47.png)

![](emnlp2020-notes/assets/images/2020-11-20-18-25-37.png)

- Time performance per step
  ![](emnlp2020-notes/assets/images/2020-11-20-18-26-23.png)

- Memory performance
  ![](emnlp2020-notes/assets/images/2020-11-20-18-26-50.png)

- Summary
  ![](emnlp2020-notes/assets/images/2020-11-20-18-27-10.png)

## Use cases

### Language models

- Evolved transformer : Uses neural architecture search to improve structure, same performance, 38% smaller
- PRADO :
  ![](emnlp2020-notes/assets/images/2020-11-20-18-35-44.png)
- Lite Transformer Long-Short range attention : long distance by attention, local context by convolution
  2.5x reduced computation, 18x smaller, no need for 250 GPU year NAS cost
- .. A bunch of others

### Retrieval

Data as integral parts of models, ability to retrieve it during inference

- Sentence-BERT : siamese networks with BERT for better retrieval
  ![](emnlp2020-notes/assets/images/2020-11-20-18-42-31.png)
  With classic sentence A [ SEP ] sentence B, would need to reencode the whole dataset at inference -> separate encoders and cosine distance
  ![](emnlp2020-notes/assets/images/2020-11-20-18-44-36.png)

Most similar datapoints can be found in sublinear time using Maximum inner product search tools
![](emnlp2020-notes/assets/images/2020-11-20-18-46-06.png)

- Generalization through Memorization : NN LMs
  Interpolate BERT with k-NN
  ![](emnlp2020-notes/assets/images/2020-11-20-18-48-15.png)
  ![](emnlp2020-notes/assets/images/2020-11-20-18-49-05.png)

- REALM : Retrieval-Augmented LM Pretraining
  Can store knowledge by storing it in network but limited by network size
  Use dual encoder from above to retrieve data
  ![](emnlp2020-notes/assets/images/2020-11-20-18-51-13.png)

Inference
![](emnlp2020-notes/assets/images/2020-11-20-18-51-57.png)
SoTA on Open-QA

Can be better and more performant with retrieval
![](emnlp2020-notes/assets/images/2020-11-20-18-53-41.png)

## Scaling in practice

### Hardware

Scale more important than architecture but attention is better
![](emnlp2020-notes/assets/images/2020-11-20-18-55-22.png)

(Attention size saturation might be due to internet data WebCrawl which has a relatively small attention span)
![](emnlp2020-notes/assets/images/2020-11-20-18-57-13.png)

Fully connected layers consume most of the compute
![](emnlp2020-notes/assets/images/2020-11-20-18-59-09.png)

Theoretical vs experimental perspective :
![](emnlp2020-notes/assets/images/2020-11-20-19-04-39.png)

Argues for not using FLOPS but wall time

Theory vs practice, consider hardware you run on :
![](emnlp2020-notes/assets/images/2020-11-20-19-05-49.png)

Other example : EfficientNet is more efficient in FLOPS but runs slower than other image algorithms

For Tensor cores on GPU : mat mul low utilization of FLOPs
![](emnlp2020-notes/assets/images/2020-11-21-17-43-46.png)

Sparsity might be needing 10x less FLOPs but 5x slower than dense so only 2x speedup
![](emnlp2020-notes/assets/images/2020-11-21-17-44-53.png)

Trade-off but no perf gains
![](emnlp2020-notes/assets/images/2020-11-21-17-45-47.png)

BERT base is too small to utilize full GPU
![](emnlp2020-notes/assets/images/2020-11-21-17-46-20.png)

Mat mul can be more efficient if we use less of the GPU : can increase resources per subcore within SM to speedup
![](emnlp2020-notes/assets/images/2020-11-21-17-48-01.png)

Conclusion
![](emnlp2020-notes/assets/images/2020-11-21-17-48-33.png)

### Memory optimization

How to fit models into memory?
![](emnlp2020-notes/assets/images/2020-11-21-17-50-02.png)

- CPU / GPU memory swapping
  e.g. only have one hot layer, swap to CPU what is not needed
  ![](emnlp2020-notes/assets/images/2020-11-21-17-51-18.png)

Benefits for large batch sizes
![](emnlp2020-notes/assets/images/2020-11-21-17-51-48.png)

- Mixed precision training :
  Multiply output loss by scale to avoid over/underflow
  ![](emnlp2020-notes/assets/images/2020-11-21-17-53-05.png)
  BF16 on newer GPUs : no need for loss scaling

- Gradient checkpointing :
  ![](emnlp2020-notes/assets/images/2020-11-21-17-56-52.png)

Trade memory for computations :
![](emnlp2020-notes/assets/images/2020-11-21-17-57-29.png)

pytorch easy to use : torch.utils.checkpoint

- Reversible residual connections
  Usually, prefer gradient checkpointing
  ![](emnlp2020-notes/assets/images/2020-11-21-17-59-05.png)

- Gradient accumulation
  Simulate training of larger batch sizes using small batch sizes
  ![](emnlp2020-notes/assets/images/2020-11-21-18-00-05.png)

### Parallelism

- Data parallelism
  Distribute gradient computations, gather them on all devices and do the update
  ![](emnlp2020-notes/assets/images/2020-11-21-18-02-32.png)

- Model parallelism
  Aplit layers across devices
  Efficient if output of model is small but layers have lots of weights : good for feedforward, bad for attention
  ![](emnlp2020-notes/assets/images/2020-11-21-18-03-23.png)

- Pipeline parallelism
  Model as pipeline and pass data to next device when done
  ![](emnlp2020-notes/assets/images/2020-11-21-18-05-07.png)

- ZeRO parallelism opt
  ![](emnlp2020-notes/assets/images/2020-11-21-18-07-01.png)

- 3D parallelism
  All parallelisms combined, can train 1 trillion parameter models with relatively few devices
  ![](emnlp2020-notes/assets/images/2020-11-21-18-08-47.png)

### Efficiency optimizations

- Larger batch sizes
  Enables larger learning rates, speeds up training
  ![](emnlp2020-notes/assets/images/2020-11-21-18-10-23.png)

- Fused kernels
  e.g. during Adam : lots of load / store operations => load all things we need and then compute
  ![](emnlp2020-notes/assets/images/2020-11-21-18-11-36.png)

### Mixture of Experts

Very large models that can be scaled efficiently
Can be 1000x more efficient than Transformers

MoE layer:
Split feedfoward layer into "experts" and select a few, only a few are active. Use gating mechanism to choose the experts. KeepTopK + Softmax, only push through selected experts and add them up
![](emnlp2020-notes/assets/images/2020-11-21-18-13-54.png)

MoE scales very well, 600B parameters (1024 experts), 1000x more efficient than GPT3
![](emnlp2020-notes/assets/images/2020-11-21-18-15-36.png)

Problem : balancing experts, poor utilization

Over time noise decreases and converges to few experts, needs counterbalancing over time, use weight to capture variation over time :
![](emnlp2020-notes/assets/images/2020-11-21-18-17-21.png)

Simpler version :
Scale loss of gate probabilities proportional to how often an expert is picked to rebalance
![](emnlp2020-notes/assets/images/2020-11-21-18-23-20.png)
![](emnlp2020-notes/assets/images/2020-11-22-00-15-48.png)

MoE is difficult :
![](emnlp2020-notes/assets/images/2020-11-22-00-17-11.png)

MoE can be very efficient but can be difficult to get right
![](emnlp2020-notes/assets/images/2020-11-22-00-20-29.png)

