---
id: 72bc9559-9ccf-41f9-952f-a031c81f2a20
title: Day2
desc: ''
updated: 1605650943052
created: 1605646514618
parent: 1977e2ad-5e1e-4f0e-94a1-b68059392e5e
children: []
fname: papers.classification.day2
hpath: papers.classification.day2
---
# Day2

## Be More with Less: Hypergraph Attention Networks for Inductive Text Classification

Kaize Ding, Jianling Wang, Jundong Li, Dingcheng Li, Huan Liu 

<https://slideslive.com/38938658>
<https://www.aclweb.org/anthology/2020.emnlp-main.399>

There exist graph-based text classification methods like TextGCN
![](emnlp2020-notes/assets/images/2020-11-17-21-57-00.png)
Learns representations of word nodes and document nodes

Has limitations :

- Expressive power, word interactions are not necessarily pairwise, could be higher order : "eat humble pie" means to admit that one was wrong
- Computational consumption huge, needs to be retrained for newly added data

Idea : Document level hypergraphs
2 types of hyperedges : 

- each sentence as hyperedge, all the words in sentence
- semantic hyperedge : each topic as hyperedge, top K words under topic distribution
  Node + edge attention + mean pooling for text classification

Comparison with CNN, LSTM, fasttext

## CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial Text Generation

Tianlu Wang, Xuezhi Wang, Yao Qin, Ben Packer, Kang Li, Jilin Chen, Alex Beutel, Ed Chi 

NLP models have robustness issues, by flipping words it can easily flip prediction

Previous works :

- word substitution by iteratively searching synonyms : lacks diversity
- GAN based methods : unrelated to input sentences

Idea : controlled adversarial text generation with encoder -> z -> decoder schema

Use projection from attributes into z to be able to perturb z and decode new sentences

