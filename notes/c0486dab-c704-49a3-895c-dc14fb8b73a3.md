---
id: c0486dab-c704-49a3-895c-dc14fb8b73a3
title: Day2
desc: ''
updated: 1606047673426
created: 1605620293636
parent: 6571e39d-d925-403b-8f7a-cfa2749cb1a3
children: []
fname: papers.interpret.day2
hpath: papers.interpret.day2
---
# Day2

## Interpretation of NLP models through input marginalization

Siwon Kim, Jihun Yi, Eunji Kim, Sungroh Yoon

<https://slideslive.com/38938947>
<https://www.aclweb.org/anthology/2020.emnlp-main.255>

How to explain influence of each word in sentence on classification?

Can replace each word by pad and check its influence on class probability
Problem : If we pad in the middle of sentence, the sentence can be out of distribution.

Idea : marginalize along the pad, instead of replacing by pad we replace by several words and average with the word probabilities as weights.

Use feedforwarding + MLM to compute the marginalization

Marginalize only over likely candidates

Apparent slightly improved interpretability

## A Diagnostic Study of Explainability Techniques for Text Classification

Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein 

<https://slideslive.com/38938813>
<https://www.aclweb.org/anthology/2020.emnlp-main.263>

Increasing number of explainability techniques, how to choose?

![](/assets/images/2020-11-17-14-50-14.png)

Propose diagnostic properties with metrics to choose :

- Human agreement
- Confidence indication : measure if saliency scores can be used to measure model performance
- Faithfulness : Masking and observing change in output
- Rationale consistency : Does an explainability technique give similar explanations for different models (different seeds)
- Dataset consistency : Same as above but for similar datapoints instead of different models
- Compute time

![](/assets/images/2020-11-17-14-56-18.png)

![](/assets/images/2020-11-17-14-58-38.png)

However LIME performs well also on different tasks for transformers

## How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking

Nicola De Cao, Michael Sejr Schlichtkrull, Wilker Aziz, Ivan Titov

Abstract : Attribution methods assess the contribution of inputs to the model prediction. One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the prediction. Though conceptually simple, erasures objective is intractable and approximate search remains expensive with modern deep NLP models. Erasure is also susceptible to the hindsight bias: the fact that an input can be dropped does not mean that the model `knows&#39; it can be dropped. The resulting pruning is over-aggressive and does not reflect how the model arrives at the prediction. To deal with these challenges, we introduce Differentiable Masking. DiffMask learns to mask-out subsets of the input while maintaining differentiability. The decision to include or disregard an input token is made with a simple model based on intermediate hidden layers of the analyzed model. First, this makes the approach efficient because we predict rather than search. Second, as with probing classifiers, this reveals what the network `knows at the corresponding layers. This lets us not only plot attribution heatmaps but also analyze how decisions are formed across network layers. We use DiffMask to study BERT models on sentiment classification and question answering.

![](/assets/images/2020-11-22-13-15-42.png)
[interesting](8c716ab6-e253-4b05-8167-ad399382adbb)

![](/assets/images/2020-11-17-15-47-04.png)

Learn mask on input tokens : optimize for output distribution between masked / unmasked
Promote sparsity : L1 on mask
Lagrange multipliers

Out of scope metrics :
![](/assets/images/2020-11-17-16-08-02.png)

## Learning Variational Word Masks to Improve the Interpretability of Neural Text Classifiers

Hanjie Chen, Yangfeng Ji 

<https://slideslive.com/38939149>
<https://www.aclweb.org/anthology/2020.emnlp-main.347>

Weakness of previous interpretability models : network architecture can make interpretability bad with previous interpretability tools e.g. LIME

Build interpretable model :
Teach model to focus on important words to make predictions

- Automatically learn task-specific important words
- Improve interpretability and performance

VMASK generates mask for each input word right after the initial embedding layer which we freeze
Use information bottleneck to keep important predictive information but remove redundant information from input

![](/assets/images/2020-11-17-16-45-09.png)

<https://github.com/UVa-NLP/VMASK>

