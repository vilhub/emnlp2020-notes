---
id: 450266f8-502d-4d30-a104-0cde447ab1f8
title: Day1
desc: ''
updated: 1605571907085
created: 1605570768846
parent: 40fbde60-26fc-4182-b55f-fae3e43b4074
children: []
fname: papers.dialog.day1
hpath: papers.dialog.day1
---
# Day1

## TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue

Chien-Sheng Wu, Steven C.H. Hoi, Richard Socher, Caiming Xiong 

<https://slideslive.com/38938861>
<https://www.aclweb.org/anthology/2020.emnlp-main.66>

BERT less good with dialogue, could be because conversational text different from writing

Use MLM + Responsive Contrastive Loss (RCL)

RCL is negative sampling to determine whether two pieces of dialogue fit together
Take 2 pieces of dialogue and use same encoder to fit them,
use in-batch negative training

Special tokens for dialogue capturing such as USR

Downstream tasks : intent recognition, dialogue state tracking, dialogue act prediction, response selection
TOD-BERT improves dialogue downstream tasks, especially for few shot scenarios

TOD-BERT available in transformers pretrained models

