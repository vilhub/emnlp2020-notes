---
id: ef4348a5-1177-4d93-a8e7-018dea2dfa3d
title: Day1
desc: ''
updated: 1606047673425
created: 1605536707504
parent: 1977e2ad-5e1e-4f0e-94a1-b68059392e5e
children: []
fname: papers.classification.day1
hpath: papers.classification.day1
---
# Interpretability and Analysis of Models for NLP

## Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually)

<https://slideslive.com/38939219>
<https://www.aclweb.org/anthology/2020.emnlp-main.16>

Idea : construct different datasets to probe Roberta's behaviour in terms of feature preference

- Test whether the models prefer linguistic or surface features

- Linguistic features : verb with "-ing", does the noun have an adjective, ..

- Surface features : Is the first token "the", length of sentence, ..

- Test out of domain generalization

- Result : Surface features are represented well by all models, linguistic features are only captured by models with more pretrained data

- Experiment : Ambiguous training data where model could use either feature : label 1 always starts with "the" and with the verb always in "-ing", which rule does the model choose on unseen data?

![](emnlp2020-notes/assets/images/2020-11-16-15-35-23.png)

- Result : As amount of pretrained data, they start showing preference for linguistic features

- Experiment : Introduce small amount of data contradicting with surface feature but consistent with linguistic generalization to nudge it to the right direction.

- Result : This nudges the models towards linguistic generalization

![](emnlp2020-notes/assets/images/2020-11-16-15-40-11.png)

![](emnlp2020-notes/assets/images/2020-11-22-13-15-42.png)
[interesting](8c716ab6-e253-4b05-8167-ad399382adbb)

