---
id: af4d40fb-5e41-460e-ae29-8a273b68c820
title: Blackbox NLP Workshop
desc: ''
updated: 1606044773813
created: 1605866426401
parent: 7070ae65-b6fc-4d6d-933f-e72b95614dc6
children: []
fname: papers.blackboxnlp
hpath: papers.blackboxnlp
---
# Blackbox NLP workshop

## On the Sub-layer Functionalities of Transformer Decoder

Yilin Yang, Longyue Wang, Shuming Shi, Prasad Tadepalli, Stefan Lee, Zhaopeng Tu 

<https://slideslive.com/38940141>
<https://www.aclweb.org/anthology/2020.findings-emnlp.432>

[gswtranslation](6ea22ede-ae7b-4746-b548-2a60e608c4f7)

BERT : lower layers capture syntactic information, higher level ones semantic information

Decoder analysis in NMT : sub-layer split
![](/assets/images/2020-11-20-11-03-37.png)

Probing decoder : takes IFM / SEM / TEM representation and force decoding of source or target tokens, compute perplexity.

![](/assets/images/2020-11-20-11-17-03.png)

![](/assets/images/2020-11-20-11-18-00.png)

- Removing TEM : essential for word alignment, don't remove

- **First FF layer in IFM and second FF layer contain same roughly same information**
  ![](/assets/images/2020-11-20-11-20-01.png)

Remove feed-forward and add-norm

## What Happens To BERT Embeddings During Fine-tuning?

Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, Ian Tenney 

<https://slideslive.com/38939763>
<https://www.aclweb.org/anthology/2020.blackboxnlp-1.4>

Using probing techniques, following conclusions
![](/assets/images/2020-11-20-11-40-18.png)

## Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation

Rajiv Movva, Jason Zhao 

<https://slideslive.com/38939765>
<https://www.aclweb.org/anthology/2020.blackboxnlp-1.19>

Transformers are very prune-able

Here use Iterative magnitude pruning (IMP) with learning rate rewinding, up to 70% sparsity

Different sparsity models :
![](/assets/images/2020-11-20-11-45-38.png)

Explore probe performance for the varying sparsity models :
![](/assets/images/2020-11-20-11-46-36.png)

However, probe architecture matters and may bias results

Conclusions :
![](/assets/images/2020-11-20-11-51-04.png)

