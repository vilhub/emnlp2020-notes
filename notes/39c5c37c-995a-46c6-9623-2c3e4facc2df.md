---
id: 39c5c37c-995a-46c6-9623-2c3e4facc2df
title: Day3
desc: ''
updated: 1606047673427
created: 1605782596879
parent: 1977e2ad-5e1e-4f0e-94a1-b68059392e5e
children: []
fname: papers.classification.day3
hpath: papers.classification.day3
---
# Day3

## Text Classification Using Label Names Only: A Language Model Self-Training Approach

Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong, Heng Ji, Chao Zhang, Jiawei Han 

<https://slideslive.com/38938946>
<https://www.aclweb.org/anthology/2020.emnlp-main.724>

![](/assets/images/2020-11-19-11-44-40.png)

Unsupervised learning given category names
![](/assets/images/2020-11-22-13-15-42.png)
[interesting](8c716ab6-e253-4b05-8167-ad399382adbb)

1. Get list of related words to a label name by using MLM and replacing whenever the label word appears in corpus
   ![](/assets/images/2020-11-19-11-45-38.png)
   ![](/assets/images/2020-11-19-11-53-13.png)
2. Masked category prediction
   Using MLM and replacing the label word with mask, if more than 20 of the 50 label words are in the top predictions, assign that label to the document
   ![](/assets/images/2020-11-19-11-57-21.png)
   Then still using the mask and the rest of the sentence, train classifier to assign this document to the label
3. Self-training
   Iteratively use model's current predictions P to compute a target distribution Q which guides the model and use KL divergence

![](/assets/images/2020-11-19-12-01-21.png)

