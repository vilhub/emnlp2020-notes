---
id: f3bc9db2-5cbb-4cdf-9845-af076d2f77f4
title: Interpretability tutorial
desc: ''
updated: 1606048495234
created: 1605798791924
parent: 27e6f1c1-1025-4574-b056-e9e00e4b6ce5
children: []
fname: papers.tutorials.interpretability
hpath: papers.tutorials.interpretability
---
# Interpretability tutorial

<https://virtual.2020.emnlp.org/tutorial_T1.html>
(recording available soon)

![](/assets/images/2020-11-19-16-15-38.png)

Interpretation methods :
Saliency or removal of fluff until we know what caused the prediction
![](/assets/images/2020-11-19-16-19-59.png)

Look for global decision rules :
![](/assets/images/2020-11-19-16-20-44.png)

Full list :
![](/assets/images/2020-11-19-16-21-49.png)

- First two in ACL 2020 tutorial
- No comprehensive resource for baking interpretability into model

This tutorial focuses on :

- Looking at input features, global decision rules and looking at training data

## Why did my model make this prediction?

- Saliency maps :
  - Using gradients
  - Using perturbations

- Perturbations as explanations
  - Input reduction
  - Adversarial perturbations

### Saliency maps

Relative importance of each token in the input w.r.t prediction
![](/assets/images/2020-11-19-16-29-58.png)

![](/assets/images/2020-11-19-16-33-25.png)

How to do it?
![](/assets/images/2020-11-19-16-37-04.png)

#### Using gradients

Problems with gradient for saliency maps :

- Very noisy locally
  ![](/assets/images/2020-11-19-16-38-58.png)
- Saturated outputs lead to unintuitive gradients
  ![](/assets/images/2020-11-19-16-40-42.png)
- Discontinuous gradients (thresholding) problematic

How to mitigate the issues? => Don't rely on single gradient calculation

- SmoothGrad
- Integrated Gradients

SmoothGrad :
![](/assets/images/2020-11-19-16-48-33.png)

Integrate the gradients over a path from zero to input : Integrated gradients
![](/assets/images/2020-11-19-16-45-48.png)

- Have to have access to gradients
- Not customizable

#### Using perturbations

- Works with black-box models
- Allows input perturbations to be defined

Methods :

- Leave one out : Remove token/phrase and look at drop in confidence
  ![](/assets/images/2020-11-19-16-53-43.png)
  Problem : Only removing **one** token
  ![](/assets/images/2020-11-19-16-56-12.png)

- LIME :
  ![](/assets/images/2020-11-19-16-57-27.png)
  ![](/assets/images/2020-11-19-16-59-37.png)

Problems :
![](/assets/images/2020-11-19-17-02-53.png)

Linear representations can be limited : double negations
![](/assets/images/2020-11-19-17-05-25.png)

### Perturbations as explanations

- Input reduction
  ![](/assets/images/2020-11-19-17-21-57.png)
  Remove words that keep prediction the same

Examples
![](/assets/images/2020-11-19-17-23-28.png)

Problems :
![](/assets/images/2020-11-19-17-30-49.png)

- Adversarial examples in NLP
  ![](/assets/images/2020-11-19-17-31-52.png)

Are they as good as explanations?
It can detect which is the word(s) responsible for a prediction
![](/assets/images/2020-11-19-17-35-26.png)

Problems:
![](/assets/images/2020-11-19-17-37-03.png)

## What decision rule led to a prediction?

Previously talked about local explanations, what about global explanations?

See patterns in data, decision rules

- Anchors :
  ![](/assets/images/2020-11-19-17-45-21.png)
  Anchor is a simple decision rule

![](/assets/images/2020-11-19-17-47-41.png)

Computing anchors :
![](/assets/images/2020-11-19-17-50-19.png)

- Universal adversarial triggers :
  Whenever this text is present, triggers a prediction
  ![](/assets/images/2020-11-19-17-52-59.png)

Start with "the the the", backprop gradient of loss, dot product with embedding matrix and take words with highest score
![](/assets/images/2020-11-19-17-55-03.png)

Example :
![](/assets/images/2020-11-19-17-56-50.png)
Problems in datasets : nobody was associated a lot with contradiction

Pros / cons of interpretation via decision rules :

- Can identify global bugs in models / datasets
- Hard to find broad coverage rules : very specific or non-actionable

Q : how to constrain search to plausible triggers?
A : Can use LM to rate plausibility of triggers

## Which training example is responsible for a prediction?

Detect wrongly labeled data for example :
![](/assets/images/2020-11-19-18-35-05.png)

![](/assets/images/2020-11-19-18-35-57.png)

Benefits of interpretation via training data :

- We know where did we pick up patterns
- Actionable : incorrect labels, biases, annotation artifacts

More precise : Which examples, if removed, would change the loss a lot?

- What would be the prediction of test set data point x given that we remove z from training set?
  ![](/assets/images/2020-11-19-18-41-02.png)
  ![](/assets/images/2020-11-19-18-41-58.png)

Use "influence functions" :

- Principled approach for estimating influence
- Works empirically for many models
  Problems :
- Scales poorly with size of model and training data
- Theory only applies to convex setting, can struggle for neural models

Alternative method : representor point selection
![](/assets/images/2020-11-19-18-45-43.png)

Hasn't been used much in NLP but interesting research direction
![](/assets/images/2020-11-19-18-48-39.png)

Problems with data influence :

- Influential points can be uninterpretable
- Computationally expensive
- Often requires approximations that may be invalid
- How does it interact with pretrained models?

## How to implement this?

Different access requirements :
![](/assets/images/2020-11-19-19-05-35.png)

- Black box : not much to say
  ![](/assets/images/2020-11-19-19-06-25.png)

- Need gradients, vanilla gradient implementation :
  Purple (need to implement yourself)
  ![](/assets/images/2020-11-19-19-09-00.png)

![](/assets/images/2020-11-19-19-10-26.png)

Use hooks :
![](/assets/images/2020-11-19-19-12-53.png)

Get the gradients :
![](/assets/images/2020-11-19-19-13-25.png)

Intepretation method :
![](/assets/images/2020-11-19-19-16-27.png)

- Implementation of integrated gradients :
  ![](/assets/images/2020-11-19-19-18-49.png)

List of implementations :
![](/assets/images/2020-11-19-19-20-31.png)

## Open problems

- Human studies to determine usefulness of interpretability with debugging models :
  ![](/assets/images/2020-11-19-19-40-15.png)

![](/assets/images/2020-11-19-19-41-44.png)

- Human - AI collaboration
  ![](/assets/images/2020-11-19-19-48-51.png)

