---
id: 3da29f21-7b4c-4d81-b230-b38455ccbc84
title: Day2
desc: ''
updated: 1606047673424
created: 1605613410756
parent: 11e3fe8a-98b7-4bb7-8fc9-fe1c29354461
children: []
fname: papers.transformers.day2
hpath: papers.transformers.day2
---
# Day2

## Masking as an Efficient Alternative to Finetuning for Pretrained Language Models

Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, Hinrich Schütze 

<https://slideslive.com/38938867>
<https://www.aclweb.org/anthology/2020.emnlp-main.174>

Instead of finetuning a pretrained model with added linear layers, we can instead mask.

We mask entries of the W matrices in the transformer blocks (W_k, W_v, etc) and train those masks w.r.t output task.

Results in comparable performance with finetuning while reducing memory footprint a lot when you have several tasks.

Kind of similar to adapters.

## When BERT Plays the Lottery, All Tickets Are Winning

Sai Prasanna, Anna Rogers, Anna Rumshisky 

<https://slideslive.com/38939071>
<https://www.aclweb.org/anthology/2020.emnlp-main.259>

Lottery ticket hypothesis :
Dense, randomly initialized feedforward networks contain subnetworks (winning tickets) that, when trained in isolation reach a test accuracy comparable to the original network in a similar number of iterations

What about pretrained Transformers?

Questions :
Does LTH hold for BERT?
Are pruned subnetworks across seeds / tasks?
How good are good subnets and how bad are bad ones?

Evaluate on GLUE

Prunings :

- Unstructured (m-pruning) iteratively prune 10% of least magnitude weights so long as model as 90% of full model accuracy (lottery ticket paper)
- Prune complete attention heads, MLPs (s-pruning)

Results :

- m-pruning stable across seeds
- s-pruning : unstable

![](/assets/images/2020-11-17-15-26-39.png)

## On the weak link between importance and prunability of attention heads

Aakriti Budhraja, Madhura Pande, Preksha Nema, Pratyush Kumar, Mitesh M. Khapra 

<https://slideslive.com/38939353>
<https://www.aclweb.org/anthology/2020.emnlp-main.260>

[gswtranslation](6ea22ede-ae7b-4746-b548-2a60e608c4f7)

Lots of experiments of pruning heads / layers

## On Losses for Modern Language Models

Stéphane Aroca-Ouellette, Frank Rudzicz 

<https://slideslive.com/38939128>
<https://www.aclweb.org/anthology/2020.emnlp-main.403>

Next sentence prediction in BERT has been criticized and dropped in Roberta, XLNet.

What is its true effect?

NSP on pretraining :
Only useful for non inference based tasks and benefits diminish over time

Test different token level tasks for pretraining
![](/assets/images/2020-11-17-22-17-35.png)

TF and TF-IDF pretraining tasks provide good improvements
ASP and Sentence Ordering provide good improvements for inference based tasks..

![](/assets/images/2020-11-17-22-22-22.png)

How to combine tasks?
Several ways, summing losses, incrementally adding tasks, alternating between tasks, .. with or without MLM

Results : MLM should always be included, improves clearly but even better with other tasks

![](/assets/images/2020-11-17-22-25-57.png)

Combination of tasks + MLM improves results

Can beat original BERT base using less than 1/4 of original tokens

![](/assets/images/2020-11-22-13-15-42.png)
[interesting](8c716ab6-e253-4b05-8167-ad399382adbb)

Caveats : different re-scaling methods for TF-IDF, choice of distance metrics for QT, FS.

